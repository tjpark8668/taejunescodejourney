{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb0e3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 데이터 가져오기\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dbt = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9fc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 모델에 입력할 데이터 X 준비하기\n",
    "\n",
    "df_X = dbt.data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df_X = np.array(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6915e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 모델에 예측할 데이터 y 준비하기\n",
    "\n",
    "df_y = dbt.target\n",
    "df_y = np.array(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f743d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) train 데이터와 test 데이터로 분리하기\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X,\n",
    "                                                    df_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af06ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) 모델 준비하기\n",
    "\n",
    "W = np.random.rand(df_X.shape[1])\n",
    "b = np.random.rand()\n",
    "\n",
    "def model (X ,W, b):\n",
    "    predictions = 0\n",
    "    for i in range (df_X.shape[1]):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253100c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) 손실함수 loss 정의하기\n",
    "\n",
    "def MSE(a, b):\n",
    "    mse = ((a-b) ** 2).mean()\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc625120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "\n",
    "def gradient(X, W, b, y):\n",
    "    N = len(W)\n",
    "    \n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b82adee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 26960.9496\n",
      "Iteration 20 : Loss 27422.5482\n",
      "Iteration 30 : Loss 27931.4438\n",
      "Iteration 40 : Loss 28453.3614\n",
      "Iteration 50 : Loss 28986.4640\n",
      "Iteration 60 : Loss 29530.7667\n",
      "Iteration 70 : Loss 30086.4650\n",
      "Iteration 80 : Loss 30653.7839\n",
      "Iteration 90 : Loss 31232.9588\n",
      "Iteration 100 : Loss 31824.2315\n",
      "Iteration 110 : Loss 32427.8502\n",
      "Iteration 120 : Loss 33044.0691\n",
      "Iteration 130 : Loss 33673.1483\n",
      "Iteration 140 : Loss 34315.3537\n",
      "Iteration 150 : Loss 34970.9574\n",
      "Iteration 160 : Loss 35640.2370\n",
      "Iteration 170 : Loss 36323.4767\n",
      "Iteration 180 : Loss 37020.9663\n",
      "Iteration 190 : Loss 37733.0020\n",
      "Iteration 200 : Loss 38459.8864\n",
      "Iteration 210 : Loss 39201.9283\n",
      "Iteration 220 : Loss 39959.4431\n",
      "Iteration 230 : Loss 40732.7528\n",
      "Iteration 240 : Loss 41522.1862\n",
      "Iteration 250 : Loss 42328.0789\n",
      "Iteration 260 : Loss 43150.7736\n",
      "Iteration 270 : Loss 43990.6201\n",
      "Iteration 280 : Loss 44847.9757\n",
      "Iteration 290 : Loss 45723.2048\n",
      "Iteration 300 : Loss 46616.6798\n",
      "Iteration 310 : Loss 47528.7807\n",
      "Iteration 320 : Loss 48459.8953\n",
      "Iteration 330 : Loss 49410.4198\n",
      "Iteration 340 : Loss 50380.7583\n",
      "Iteration 350 : Loss 51371.3236\n",
      "Iteration 360 : Loss 52382.5371\n",
      "Iteration 370 : Loss 53414.8287\n",
      "Iteration 380 : Loss 54468.6377\n",
      "Iteration 390 : Loss 55544.4122\n",
      "Iteration 400 : Loss 56642.6098\n",
      "Iteration 410 : Loss 57763.6976\n",
      "Iteration 420 : Loss 58908.1525\n",
      "Iteration 430 : Loss 60076.4612\n",
      "Iteration 440 : Loss 61269.1208\n",
      "Iteration 450 : Loss 62486.6385\n",
      "Iteration 460 : Loss 63729.5322\n",
      "Iteration 470 : Loss 64998.3306\n",
      "Iteration 480 : Loss 66293.5733\n",
      "Iteration 490 : Loss 67615.8114\n",
      "Iteration 500 : Loss 68965.6072\n",
      "Iteration 510 : Loss 70343.5349\n",
      "Iteration 520 : Loss 71750.1806\n",
      "Iteration 530 : Loss 73186.1427\n",
      "Iteration 540 : Loss 74652.0319\n",
      "Iteration 550 : Loss 76148.4718\n",
      "Iteration 560 : Loss 77676.0990\n",
      "Iteration 570 : Loss 79235.5633\n",
      "Iteration 580 : Loss 80827.5279\n",
      "Iteration 590 : Loss 82452.6701\n",
      "Iteration 600 : Loss 84111.6811\n",
      "Iteration 610 : Loss 85805.2667\n",
      "Iteration 620 : Loss 87534.1472\n",
      "Iteration 630 : Loss 89299.0582\n",
      "Iteration 640 : Loss 91100.7502\n",
      "Iteration 650 : Loss 92939.9897\n",
      "Iteration 660 : Loss 94817.5591\n",
      "Iteration 670 : Loss 96734.2570\n",
      "Iteration 680 : Loss 98690.8988\n",
      "Iteration 690 : Loss 100688.3168\n",
      "Iteration 700 : Loss 102727.3606\n",
      "Iteration 710 : Loss 104808.8975\n",
      "Iteration 720 : Loss 106933.8130\n",
      "Iteration 730 : Loss 109103.0111\n",
      "Iteration 740 : Loss 111317.4144\n",
      "Iteration 750 : Loss 113577.9648\n",
      "Iteration 760 : Loss 115885.6241\n",
      "Iteration 770 : Loss 118241.3737\n",
      "Iteration 780 : Loss 120646.2158\n",
      "Iteration 790 : Loss 123101.1734\n",
      "Iteration 800 : Loss 125607.2907\n",
      "Iteration 810 : Loss 128165.6338\n",
      "Iteration 820 : Loss 130777.2909\n",
      "Iteration 830 : Loss 133443.3730\n",
      "Iteration 840 : Loss 136165.0143\n",
      "Iteration 850 : Loss 138943.3723\n",
      "Iteration 860 : Loss 141779.6291\n",
      "Iteration 870 : Loss 144674.9910\n",
      "Iteration 880 : Loss 147630.6897\n",
      "Iteration 890 : Loss 150647.9825\n",
      "Iteration 900 : Loss 153728.1530\n",
      "Iteration 910 : Loss 156872.5112\n",
      "Iteration 920 : Loss 160082.3949\n",
      "Iteration 930 : Loss 163359.1694\n",
      "Iteration 940 : Loss 166704.2286\n",
      "Iteration 950 : Loss 170118.9954\n",
      "Iteration 960 : Loss 173604.9224\n",
      "Iteration 970 : Loss 177163.4924\n",
      "Iteration 980 : Loss 180796.2193\n",
      "Iteration 990 : Loss 184504.6482\n",
      "Iteration 1000 : Loss 188290.3568\n"
     ]
    }
   ],
   "source": [
    "# (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "\n",
    "LEARNING_RATE = 1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35fe2a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 2835.2483\n",
      "Iteration 20 : Loss 2835.2480\n",
      "Iteration 30 : Loss 2835.2477\n",
      "Iteration 40 : Loss 2835.2474\n",
      "Iteration 50 : Loss 2835.2471\n",
      "Iteration 60 : Loss 2835.2468\n",
      "Iteration 70 : Loss 2835.2466\n",
      "Iteration 80 : Loss 2835.2463\n",
      "Iteration 90 : Loss 2835.2460\n",
      "Iteration 100 : Loss 2835.2457\n",
      "Iteration 110 : Loss 2835.2454\n",
      "Iteration 120 : Loss 2835.2451\n",
      "Iteration 130 : Loss 2835.2448\n",
      "Iteration 140 : Loss 2835.2445\n",
      "Iteration 150 : Loss 2835.2442\n",
      "Iteration 160 : Loss 2835.2439\n",
      "Iteration 170 : Loss 2835.2437\n",
      "Iteration 180 : Loss 2835.2434\n",
      "Iteration 190 : Loss 2835.2431\n",
      "Iteration 200 : Loss 2835.2428\n",
      "Iteration 210 : Loss 2835.2425\n",
      "Iteration 220 : Loss 2835.2422\n",
      "Iteration 230 : Loss 2835.2419\n",
      "Iteration 240 : Loss 2835.2416\n",
      "Iteration 250 : Loss 2835.2414\n",
      "Iteration 260 : Loss 2835.2411\n",
      "Iteration 270 : Loss 2835.2408\n",
      "Iteration 280 : Loss 2835.2405\n",
      "Iteration 290 : Loss 2835.2402\n",
      "Iteration 300 : Loss 2835.2399\n",
      "Iteration 310 : Loss 2835.2397\n",
      "Iteration 320 : Loss 2835.2394\n",
      "Iteration 330 : Loss 2835.2391\n",
      "Iteration 340 : Loss 2835.2388\n",
      "Iteration 350 : Loss 2835.2385\n",
      "Iteration 360 : Loss 2835.2382\n",
      "Iteration 370 : Loss 2835.2380\n",
      "Iteration 380 : Loss 2835.2377\n",
      "Iteration 390 : Loss 2835.2374\n",
      "Iteration 400 : Loss 2835.2371\n",
      "Iteration 410 : Loss 2835.2368\n",
      "Iteration 420 : Loss 2835.2366\n",
      "Iteration 430 : Loss 2835.2363\n",
      "Iteration 440 : Loss 2835.2360\n",
      "Iteration 450 : Loss 2835.2357\n",
      "Iteration 460 : Loss 2835.2355\n",
      "Iteration 470 : Loss 2835.2352\n",
      "Iteration 480 : Loss 2835.2349\n",
      "Iteration 490 : Loss 2835.2346\n",
      "Iteration 500 : Loss 2835.2344\n",
      "Iteration 510 : Loss 2835.2341\n",
      "Iteration 520 : Loss 2835.2338\n",
      "Iteration 530 : Loss 2835.2335\n",
      "Iteration 540 : Loss 2835.2333\n",
      "Iteration 550 : Loss 2835.2330\n",
      "Iteration 560 : Loss 2835.2327\n",
      "Iteration 570 : Loss 2835.2324\n",
      "Iteration 580 : Loss 2835.2322\n",
      "Iteration 590 : Loss 2835.2319\n",
      "Iteration 600 : Loss 2835.2316\n",
      "Iteration 610 : Loss 2835.2314\n",
      "Iteration 620 : Loss 2835.2311\n",
      "Iteration 630 : Loss 2835.2308\n",
      "Iteration 640 : Loss 2835.2305\n",
      "Iteration 650 : Loss 2835.2303\n",
      "Iteration 660 : Loss 2835.2300\n",
      "Iteration 670 : Loss 2835.2297\n",
      "Iteration 680 : Loss 2835.2295\n",
      "Iteration 690 : Loss 2835.2292\n",
      "Iteration 700 : Loss 2835.2289\n",
      "Iteration 710 : Loss 2835.2287\n",
      "Iteration 720 : Loss 2835.2284\n",
      "Iteration 730 : Loss 2835.2281\n",
      "Iteration 740 : Loss 2835.2279\n",
      "Iteration 750 : Loss 2835.2276\n",
      "Iteration 760 : Loss 2835.2273\n",
      "Iteration 770 : Loss 2835.2271\n",
      "Iteration 780 : Loss 2835.2268\n",
      "Iteration 790 : Loss 2835.2265\n",
      "Iteration 800 : Loss 2835.2263\n",
      "Iteration 810 : Loss 2835.2260\n",
      "Iteration 820 : Loss 2835.2257\n",
      "Iteration 830 : Loss 2835.2255\n",
      "Iteration 840 : Loss 2835.2252\n",
      "Iteration 850 : Loss 2835.2250\n",
      "Iteration 860 : Loss 2835.2247\n",
      "Iteration 870 : Loss 2835.2244\n",
      "Iteration 880 : Loss 2835.2242\n",
      "Iteration 890 : Loss 2835.2239\n",
      "Iteration 900 : Loss 2835.2236\n",
      "Iteration 910 : Loss 2835.2234\n",
      "Iteration 920 : Loss 2835.2231\n",
      "Iteration 930 : Loss 2835.2229\n",
      "Iteration 940 : Loss 2835.2226\n",
      "Iteration 950 : Loss 2835.2223\n",
      "Iteration 960 : Loss 2835.2221\n",
      "Iteration 970 : Loss 2835.2218\n",
      "Iteration 980 : Loss 2835.2216\n",
      "Iteration 990 : Loss 2835.2213\n",
      "Iteration 1000 : Loss 2835.2211\n"
     ]
    }
   ],
   "source": [
    "# (9) 모델 학습하기\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72aec996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998.8760238357777"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10) test 데이터에 대한 성능 확인하기\n",
    "\n",
    "loss(X_test, W, b, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c00d76e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model(X_test ,W, b)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_test[:,0], y_pred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
